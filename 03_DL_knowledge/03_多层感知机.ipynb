{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 多层感知机\n",
    "_定义_：多层感知机是采用多个神经网络层，并在其中添加了非线性单元，从而使得网络可以得到从输入到输出的非线性映射，从而将线性模型转化为非线性模型，在此基础上，增加网络的深度可以进一步增加网络的复杂度。\n",
    "\n",
    "## 激活函数：\n",
    "_定义_：拥有非线性部分的函数，即函数的导数并非处处一致，常见的激活函数有Relu，Leaky-relu，tanh，sigmoid等\n",
    "\n",
    "实例：\n",
    "\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "![Untitled/Untitled.png](Pics/activation.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "## 重要的问题：\n",
    "### 1. 不同的激活函数应当如何选择？\n",
    "在考虑不同激活函数的区别和优势时，我们主要考虑激活函数对神经元输出的影响和神经元的数值更新的能力。（即神经元的存在以及学习能力）\n",
    "\n",
    "*神经元的存在问题*：当一个神经元更新到无论什么参数输入，其输出均为零时，其就失活了。这个主要是因为某些激活函数的截断作用，当大量神经元失活，网络的复杂度就会受到严重影响\n",
    "\n",
    "*神经元参数的更新*：其取决于反向传播到该神经元的梯度值，这个梯度值是累乘的，其会乘上激活函数的梯度。因此，如果激活函数的梯度过小，那么就会导致这个神经元学习缓慢/无法学习\n",
    "\n",
    "\n",
    "* Relu：由于其计算效率和不容易梯度消失，从而大多数情况下使用；但是，由于其输出范围不确定，因此只能在隐藏层中使用；同时Relu函数因为在小于0的部分为0，因此容易在学习率大时使神经元失活\n",
    "\n",
    "* Sigmoid函数：用于分类器时，通常效果更好，但是存在由于梯度消失。\n",
    "\n",
    "* Tanh 函数: 和sigmoid 相似，但是其导数输出在（0，1），相比sigmoid更容易激活神经元；也存在梯度消失的问题\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}