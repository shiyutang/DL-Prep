{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 数值稳定性与模型初始化\n",
    "通常我们希望我们的模型在训练过程中能快速地学习并在学习之后收敛到一个较好的结果。因此网络的数值稳定性就格外重要，我们希望网络中的神经元能每次更新适当大小的梯度作为优化方向。而梯度消失和梯度爆炸则会导致上述训练停滞或不能收敛。\n",
    "> 深度模型有关数值稳定性的典型问题是衰减和爆炸。当神经网络的层数较多时，模型的数值稳定性容易变差。\n",
    "\n",
    "_梯度消失_：其指的是传递到很多神经元的梯度几乎为0，从而导致网络的学习停滞\n",
    "\n",
    "_梯度爆炸_：对应于梯度消失，梯度爆炸则代表神经元的梯度过大，使得其一下更新到一个不合适的位置，从而网络不能收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 导致梯度消失和梯度爆炸的原因：\n",
    "1. 神经网络没有很好地初始化：当我们假设神经网络没有激活层时，传递到某一个神经元的梯度就是其后方连接上所有神经元参数的连乘积。那么，在较深的网络中如果我们将神经元均初始化较大（>1）的数值会导致梯度爆炸，较小（<1）则会导致梯度消失\n",
    "那么有激活函数的场景怎么样呢，有激活函数时，不同的损失函数的情况有所不同，对于Sigmoid激活函数，当参数初始化较小的值，神经元的输入会较小，此时激活函数对应的梯度也较小，由此影响到神经元的梯度也较小，从而造成了梯度消失。\n",
    "2. 没有选择恰当的激活函数：如第一点所述，不同的激活函数会对梯度消失和梯度爆炸产生影响，因此需要根据数据选择合理的激活函数\n",
    "3. 输入没有合理地归一化：影响激活函数梯度的除了其自身之外，还有其输入，即便神经元初始正常，没有归一化的输入也会导致神经元输出过大/小，从而激活函数梯度过小"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 神经元初始化的方法--缓解梯度消失和爆炸\n",
    "根据上述阐述，我们知道神经元的权重应当初始化到一个1附近的数值/0附近（对于Sigmoid等激活函数来说），因此有各种各样的初始化方法，在pytorch里面可以通过torch.init调用\n",
    "Xavier初始化：将权重参数初始化到：\n",
    "<div align=center>\n",
    "\n",
    "![Untitled/Untitled.png](Pics/Uniform.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "根据均匀分布的均值和方差公式，我们可以知道其初始化后，均值为0，方差为2/（a+b）\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}